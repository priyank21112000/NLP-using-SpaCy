{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNzXyy37PCBfYVOERd0qEn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyank21112000/NLP-using-SpaCy/blob/main/NLP_using_SpaCy_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MQa2zLIQyh1E"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MlowepGyvvL",
        "outputId": "95ecc107-5bda-442f-d543-54e1d81f443b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-28 19:01:35.574135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Vector** -\n",
        "Word vectors, or word embeddings, are numerical representations of words in multidimensional space through matrices. The purpose of the word vector is to get a computer system to understand a word. Computers cannot understand text efficiently. They can, however, process numbers quickly and well. For this reason, it is important to convert a word into a number."
      ],
      "metadata": {
        "id": "jZk8JMkazL_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "ZotJMnfny0Pm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/Rise of AI.txt\", 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "ync_ZWd3z7XV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "sentence1 = list(doc.sents)[0]\n",
        "print(sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQICNPir0UZ1",
        "outputId": "cb28fa3f-b926-44a8-ed56-64df2c870508"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Rise of Artificial Intelligence: Revolutionizing Industries and Transforming Society\\\n",
            "\n",
            "Introduction\n",
            "Artificial Intelligence (AI) has emerged as one of the most transformative and revolutionary technologies of the 21st century.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_word = \"learning\""
      ],
      "metadata": {
        "id": "aNq1bgdW0pt4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most similar words, not synonyms\n",
        "import numpy as np\n",
        "ms = nlp.vocab.vectors.most_similar(\n",
        "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[my_word]]]), n=10)\n",
        "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
        "distances = ms[2]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8COFBmsq1eCc",
        "outputId": "e16a801d-6ae8-4d44-beac-7f0539e9dd7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itslearning', 'teachin', 'Garners', 'Microlearning', 'understandingly', 'educates', 'knowles', 'renverser', 'pressroom', 'memristive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like fries and vegetarian burgers\")\n",
        "doc2 = nlp(\"Fast food tastes very good\")"
      ],
      "metadata": {
        "id": "TlYFNiwz1jR5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc1, \"<->\", doc2, \":\",doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPhsHACe2woj",
        "outputId": "365d5ab4-762a-49f8-a719-e0c44a65a39a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like fries and vegetarian burgers <-> Fast food tastes very good : 0.594179107276761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 =  nlp(\"Brooklyn bridge is a bridge in Brooklyn, New York\")\n",
        "print(doc1, \"<->\", doc3, \":\",doc1.similarity(doc3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2U5Rc-T20tP",
        "outputId": "2eb977b4-927f-46e4-c940-9c233b44367e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like fries and vegetarian burgers <-> Brooklyn bridge is a bridge in Brooklyn, New York : 0.2522421310765939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(\"I like having fruits\")\n",
        "doc5 = nlp(\"I love staying healthy\")\n",
        "print(doc4, \"<->\", doc5, \":\",doc4.similarity(doc5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGn2LsG3uhq",
        "outputId": "96e243f0-cf98-46a3-8005-e28f07d15c9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like having fruits <-> I love staying healthy : 0.8781820811565578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SpaCy Pipelines** -\n",
        " SpaCy is much more than an NLP framework. It is also a way of designing and implementing complex pipelines. A pipeline is a sequence of pipes, or actors on data, that make alterations to the data or extract information from it"
      ],
      "metadata": {
        "id": "nYAT5Fi37M5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "zTFU9sqU4njV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuTOzNGe7tiM",
        "outputId": "9d86f413-f35d-4eb2-eb3f-1f25be82ba3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x79430af13c80>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
        "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
        "nlp.max_length = 5278439"
      ],
      "metadata": {
        "id": "cRbirT6I7zkq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIowVvpz8ZVz",
        "outputId": "1657d400-a9b2-4afc-e486-8a4b56aa6ab6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'sentencizer': []},\n",
              " 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2 = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "qds5uqQ18bv2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTZRDidH9Sop",
        "outputId": "af13bb3a-e3a0-4621-fbdc-a5d1e02bffa7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'tagger': {'assigns': ['token.tag'],\n",
              "   'requires': [],\n",
              "   'scores': ['tag_acc'],\n",
              "   'retokenizes': False},\n",
              "  'parser': {'assigns': ['token.dep',\n",
              "    'token.head',\n",
              "    'token.is_sent_start',\n",
              "    'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['dep_uas',\n",
              "    'dep_las',\n",
              "    'dep_las_per_type',\n",
              "    'sents_p',\n",
              "    'sents_r',\n",
              "    'sents_f'],\n",
              "   'retokenizes': False},\n",
              "  'attribute_ruler': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'lemmatizer': {'assigns': ['token.lemma'],\n",
              "   'requires': [],\n",
              "   'scores': ['lemma_acc'],\n",
              "   'retokenizes': False},\n",
              "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'tok2vec': [],\n",
              "  'tagger': [],\n",
              "  'parser': [],\n",
              "  'attribute_ruler': [],\n",
              "  'lemmatizer': [],\n",
              "  'ner': []},\n",
              " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
              "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
              "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entity Ruler**\n",
        "The EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. A factory in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of the EntityRuler, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities."
      ],
      "metadata": {
        "id": "OiQBYpOk-PVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two Approches -\n",
        "\n",
        "**Rule Based Approach** - In the rule-based approach, linguistic experts manually create a set of predefined rules to analyze and process natural language text. These rules are designed to identify specific patterns, syntactic structures, or semantic relationships within the text. The rules are typically based on linguistic knowledge and grammatical principles.\n",
        "\n",
        "*Use a Rule-Based Approach When*:\n",
        "\n",
        "Well-defined tasks: Rule-based approaches work well for tasks with clear and specific rules, such as simple text preprocessing, keyword matching, or basic information extraction.\n",
        "\n",
        "Domain-specific knowledge: If the task requires expertise in a particular domain, rule-based systems can be crafted by linguistic experts to capture domain-specific nuances and jargon effectively.\n",
        "\n",
        "Limited data: When training data is scarce or of poor quality, rule-based systems can be a viable alternative as they do not require large datasets for development.\n",
        "\n",
        "Interpretability and explainability: In applications where transparency and interpretability are crucial, rule-based systems offer clear and understandable processing steps."
      ],
      "metadata": {
        "id": "NsGH1cwh_PvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning-Based Approach** -\n",
        "In the machine learning-based approach, NLP models are trained on large datasets, where the model learns patterns and relationships from the data itself. These models use statistical algorithms to make predictions and derive meaning from the text. Popular machine learning techniques used in NLP include deep learning models like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers.\n",
        "\n",
        "*Use a Machine Learning-Based Approach When* :\n",
        "\n",
        "Complex tasks: Machine learning-based approaches are ideal for more complex NLP tasks, such as sentiment analysis, named entity recognition, machine translation, and language generation.\n",
        "\n",
        "Large and diverse datasets: If sufficient high-quality data is available, machine learning models can learn patterns and relationships effectively, leading to better generalization and performance.\n",
        "\n",
        "Contextual understanding: Machine learning models, especially deep learning-based ones, are well-suited for tasks that require context-dependent comprehension, such as machine comprehension or question-answering systems.\n",
        "\n",
        "Adaptability and scalability: In applications where the language patterns change frequently or adaptability to new data is essential, machine learning models can continuously improve with additional training data."
      ],
      "metadata": {
        "id": "igHJuGDkAIag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"West Chestertenfieldville was referred in Mr. Deeds.\""
      ],
      "metadata": {
        "id": "wMTJjCOS9Uio"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "lkQAeXR5HEYn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ3k18waHFYw",
        "outputId": "fbe9811b-709d-4aa3-a9d3-31c87aeacc55"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "West Chestertenfieldville GPE\n",
            "Deeds PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruler = nlp.add_pipe(\"entity_ruler\")"
      ],
      "metadata": {
        "id": "0T2KRrHwHRzt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQoC9wTQH6w4",
        "outputId": "4fd58e03-d77b-4c6c-a5bf-b8dc8b060d1b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'tagger': {'assigns': ['token.tag'],\n",
              "   'requires': [],\n",
              "   'scores': ['tag_acc'],\n",
              "   'retokenizes': False},\n",
              "  'parser': {'assigns': ['token.dep',\n",
              "    'token.head',\n",
              "    'token.is_sent_start',\n",
              "    'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['dep_uas',\n",
              "    'dep_las',\n",
              "    'dep_las_per_type',\n",
              "    'sents_p',\n",
              "    'sents_r',\n",
              "    'sents_f'],\n",
              "   'retokenizes': False},\n",
              "  'attribute_ruler': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'lemmatizer': {'assigns': ['token.lemma'],\n",
              "   'requires': [],\n",
              "   'scores': ['lemma_acc'],\n",
              "   'retokenizes': False},\n",
              "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False},\n",
              "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'tok2vec': [],\n",
              "  'tagger': [],\n",
              "  'parser': [],\n",
              "  'attribute_ruler': [],\n",
              "  'lemmatizer': [],\n",
              "  'ner': [],\n",
              "  'entity_ruler': []},\n",
              " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
              "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
              "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2 = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7ast3hkRLYxH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruler = nlp2.add_pipe(\"entity_ruler\", before=\"ner\")"
      ],
      "metadata": {
        "id": "ZzrdjcFhLhJJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\n",
        "  {\"label\": \"Film\", \"pattern\":\"Mr. Deeds\" }\n",
        "]"
      ],
      "metadata": {
        "id": "k95ba-a-Lsk7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "eIUzqNZQMEBz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp2(text)"
      ],
      "metadata": {
        "id": "KtmdSuctMHQ6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJkD1bXCMPpZ",
        "outputId": "75e27831-91e6-490a-c597-8d435946d93c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "West Chestertenfieldville GPE\n",
            "Mr. Deeds Film\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Matcher**"
      ],
      "metadata": {
        "id": "1y8sP4uHNI66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "ahXF7ji4MRMG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "5VznvKQqN63z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher=Matcher(nlp.vocab)\n",
        "pattern = [{\"LIKE_EMAIL\": True}]\n",
        "matcher.add(\"EMAIL_ADDRESS\", [pattern])"
      ],
      "metadata": {
        "id": "CwaQSz1TN_Gh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a test email: 6l6Rm@example.com\")\n",
        "matches = matcher(doc)"
      ],
      "metadata": {
        "id": "iHg1WMUdODQ_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sj_iD3GO4hi",
        "outputId": "16d766cd-86fb-4fd5-84ea-914a8df6d5f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(16571425990740197027, 6, 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.vocab[matches[0][0]].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hit0w5O8O8sj",
        "outputId": "6890fd67-d172-47b4-995c-903fc181f899"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMAIL_ADDRESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/Martin_Luther_King.txt\", \"r\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "X9_vRIKDPdeP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94LYpYzVUznr",
        "outputId": "e3c8b273-0a02-4d3b-ff95-d94cdef5cf49"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 â€“ April 4, 1968) was an American Baptist minister and activist who was one of the most prominent leaders in the civil rights movement from 1955 until his assassination on April 4, 1968. A Black church leader and a son of early civil rights activist and minister Martin Luther King Sr., King advanced civil rights for people of color in the United States through nonviolence and civil disobedience. Inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi, he led targeted, nonviolent resistance against Jim Crow laws and other forms of discrimination in the United States.\n",
            "\n",
            "King participated in and led marches for the right to vote, desegregation, labor rights, and other civil rights.[1] He oversaw the 1955 Montgomery bus boycott and later became the first president of the Southern Christian Leadership Conference (SCLC). As president of the SCLC, he led the unsuccessful Albany Movement in Albany, Georgia, and helped organize some of the nonviolent 1963 protests in Birmingham, Alabama. King was one of the leaders of the 1963 March on Washington, where he delivered his \"I Have a Dream\" speech on the steps of the Lincoln Memorial. The civil rights movement achieved pivotal legislative gains in the Civil Rights Act of 1964, Voting Rights Act of 1965, and the Fair Housing Act of 1968.\n",
            "\n",
            "The SCLC put into practice the tactics of nonviolent protest with some success by strategically choosing the methods and places in which protests were carried out. There were several dramatic standoffs with segregationist authorities, who frequently responded violently.[2] King was jailed several times. Federal Bureau of Investigation (FBI) director J. Edgar Hoover considered King a radical and made him an object of the FBI's COINTELPRO from 1963 forward. FBI agents investigated him for possible communist ties, spied on his personal life, and secretly recorded him. In 1964, the FBI mailed King a threatening anonymous letter, which he interpreted as an attempt to make him commit suicide.[3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Rhj0g5_-U4EH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\":\"PROPN\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [pattern])\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print(len(matches))\n",
        "for match in matches[:10]:\n",
        "  print(match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAgx_wWEVCNQ",
        "outputId": "f46bac2d-06f5-4731-f65d-31c560b34e38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67\n",
            "(451313080118390996, 0, 1) Martin\n",
            "(451313080118390996, 1, 2) Luther\n",
            "(451313080118390996, 2, 3) King\n",
            "(451313080118390996, 3, 4) Jr.\n",
            "(451313080118390996, 6, 7) Michael\n",
            "(451313080118390996, 7, 8) King\n",
            "(451313080118390996, 8, 9) Jr.\n",
            "(451313080118390996, 10, 11) January\n",
            "(451313080118390996, 15, 16) April\n",
            "(451313080118390996, 23, 24) Baptist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [pattern])\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print(len(matches))\n",
        "for match in matches[:10]:\n",
        "  print(match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9z4vfQ8VQB2",
        "outputId": "d05ead9e-5e83-4cd6-d9a8-38531453c003"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111\n",
            "(451313080118390996, 0, 1) Martin\n",
            "(451313080118390996, 0, 2) Martin Luther\n",
            "(451313080118390996, 1, 2) Luther\n",
            "(451313080118390996, 0, 3) Martin Luther King\n",
            "(451313080118390996, 1, 3) Luther King\n",
            "(451313080118390996, 2, 3) King\n",
            "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
            "(451313080118390996, 1, 4) Luther King Jr.\n",
            "(451313080118390996, 2, 4) King Jr.\n",
            "(451313080118390996, 3, 4) Jr.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [pattern], greedy=\"LONGEST\")\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print(len(matches))\n",
        "for match in matches[:10]:\n",
        "  print(match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UGSzfIGWmPO",
        "outputId": "21bc4b11-411f-4d3a-b01b-500a2c73ac81"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "(451313080118390996, 65, 70) Martin Luther King Sr.\n",
            "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
            "(451313080118390996, 160, 164) Southern Christian Leadership Conference\n",
            "(451313080118390996, 6, 9) Michael King Jr.\n",
            "(451313080118390996, 241, 244) Civil Rights Act\n",
            "(451313080118390996, 247, 250) Voting Rights Act\n",
            "(451313080118390996, 255, 258) Fair Housing Act\n",
            "(451313080118390996, 317, 320) J. Edgar Hoover\n",
            "(451313080118390996, 81, 83) United States\n",
            "(451313080118390996, 99, 101) Mahatma Gandhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [pattern], greedy=\"LONGEST\")\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "matches.sort(key=lambda x: x[1])\n",
        "print(len(matches))\n",
        "for match in matches[:10]:\n",
        "  print(match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qujRLPPIXKyM",
        "outputId": "350b3176-bfe6-42a2-d9b2-cf45fd85ef83"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
            "(451313080118390996, 6, 9) Michael King Jr.\n",
            "(451313080118390996, 10, 11) January\n",
            "(451313080118390996, 15, 16) April\n",
            "(451313080118390996, 23, 24) Baptist\n",
            "(451313080118390996, 46, 47) April\n",
            "(451313080118390996, 65, 70) Martin Luther King Sr.\n",
            "(451313080118390996, 71, 72) King\n",
            "(451313080118390996, 81, 83) United States\n",
            "(451313080118390996, 99, 101) Mahatma Gandhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}, {\"POS\":\"VERB\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [pattern], greedy=\"LONGEST\")\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "matches.sort(key=lambda x: x[1])\n",
        "print(len(matches))\n",
        "for match in matches[:10]:\n",
        "  print(match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ahv9tYEeQBA",
        "outputId": "ffbcd408-652e-4dbb-e636-4abda83911ce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "(451313080118390996, 71, 73) King advanced\n",
            "(451313080118390996, 123, 125) King participated\n",
            "(451313080118390996, 317, 321) J. Edgar Hoover considered\n",
            "(451313080118390996, 362, 364) FBI mailed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Custom Components\n"
      ],
      "metadata": {
        "id": "46LjVAeoPKDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"New York is a place. Steve is a person.\")"
      ],
      "metadata": {
        "id": "OjTO6F-24Zw9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc2.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPtgdILdPmCD",
        "outputId": "340ee56e-b7bf-4eb6-ff9f-733d47b8c810"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York GPE\n",
            "Steve PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language"
      ],
      "metadata": {
        "id": "BQDwNqvdPqd0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"remove_GPE\")\n",
        "def remove_GPE(doc2):\n",
        "  original_ents = list(doc2.ents)\n",
        "  for ent in doc2.ents:\n",
        "    if ent.label_ == \"GPE\":\n",
        "      original_ents.remove(ent)\n",
        "  doc2.ents = original_ents\n",
        "  return doc2\n"
      ],
      "metadata": {
        "id": "oWTTzCuuP2-c"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"remove_GPE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TET0i6iZQ4p1",
        "outputId": "c66de58e-d4d4-4b3f-9edf-5f7c5edf3c21"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.remove_GPE(doc2)>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzsCAMtvQ9EH",
        "outputId": "0230f413-992b-40c7-da8d-eabcb8c63e36"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'tagger': {'assigns': ['token.tag'],\n",
              "   'requires': [],\n",
              "   'scores': ['tag_acc'],\n",
              "   'retokenizes': False},\n",
              "  'parser': {'assigns': ['token.dep',\n",
              "    'token.head',\n",
              "    'token.is_sent_start',\n",
              "    'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['dep_uas',\n",
              "    'dep_las',\n",
              "    'dep_las_per_type',\n",
              "    'sents_p',\n",
              "    'sents_r',\n",
              "    'sents_f'],\n",
              "   'retokenizes': False},\n",
              "  'attribute_ruler': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'lemmatizer': {'assigns': ['token.lemma'],\n",
              "   'requires': [],\n",
              "   'scores': ['lemma_acc'],\n",
              "   'retokenizes': False},\n",
              "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False},\n",
              "  'remove_GPE': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'tok2vec': [],\n",
              "  'tagger': [],\n",
              "  'parser': [],\n",
              "  'attribute_ruler': [],\n",
              "  'lemmatizer': [],\n",
              "  'ner': [],\n",
              "  'remove_GPE': []},\n",
              " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
              "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
              "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
              "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"New York is a place. Steve is a person.\")\n",
        "for ent in doc2.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7koR4R7sRMdC",
        "outputId": "58c3fa1f-cef9-427e-e2ca-824163ef1018"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions"
      ],
      "metadata": {
        "id": "g5yl9-74V27m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "FZfrwE-vRXF3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paul Neuman was an American actor, but Paul Hollywood is a British TV host. The name Paul is quite common.\""
      ],
      "metadata": {
        "id": "46cC_CL1V7RA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"Paul [A-Z]\\w+\""
      ],
      "metadata": {
        "id": "oqh8NUSNWUSA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = re.finditer(pattern, text)\n",
        "for match in matches:\n",
        "  print(match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjiMH5fuWmHd",
        "outputId": "29c60e17-b877-4b29-8f78-4669e43ef4f1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 11), match='Paul Neuman'>\n",
            "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "70RNeUU5WxN1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc3 = nlp(text)\n",
        "original_ents = list(doc3.ents)\n",
        "mwt_ents = []\n",
        "for match in re.finditer(pattern, text):\n",
        "  start, end = match.span()\n",
        "  span = doc3.char_span(start, end)\n",
        "  if span is not None:\n",
        "    mwt_ents.append((span.start, span.end, span.text))"
      ],
      "metadata": {
        "id": "f_cqHDrpXUNZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mwt_ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qexcByyBYQS8",
        "outputId": "8e4ab283-8cfd-42c1-d782-d6258322bf5a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 2, 'Paul Neuman'), (8, 10, 'Paul Hollywood')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UI7ZTJ6rYuji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}